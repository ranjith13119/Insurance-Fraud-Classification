{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Insurance Fraud Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "19OSl65MTh36GdjL6TYb83faohoAwQ3eE",
      "authorship_tag": "ABX9TyNUGzKHHarZvl1M9pTrt/Rq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranjith13119/Insurance-Fraud-Classification/blob/main/Insurance_Fraud_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCvxI5tNl0XI"
      },
      "source": [
        "# import os\n",
        "# os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/Kaggle\"\n",
        "# %cd /content/drive/MyDrive/Insurance Fraud Detection\n",
        "# !kaggle datasets download -d buntyshah/auto-insurance-claims-data\n",
        "# #unzipping the zip files and deleting the zip files\n",
        "# !unzip \\*.zip  && rm *.zipy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8bXoUVVjH7-"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjkzpEYUso5h",
        "outputId": "f3d15138-5009-421c-beff-f006121460e4"
      },
      "source": [
        "from sklearn_pandas import CategoricalImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.combine import SMOTETomek\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1igcLwT1oQ3g"
      },
      "source": [
        "class Preprocessor:\n",
        "  def __init__(self, df):\n",
        "    self.data = df\n",
        "\n",
        "  def remove_unwanted_spaces(self):\n",
        "    try:\n",
        "      self.df_without_space = self.data.apply(lambda x : x.str.strip() if x.dtype == 'object' else x)\n",
        "      return self.df_without_space\n",
        "    except Exception as err:\n",
        "      raise Exception()\n",
        "  \n",
        "  def remove_columns(self, data, columns):\n",
        "    self.data = data\n",
        "    self.remove_col = columns\n",
        "    try:\n",
        "      self.useful_data = self.data.drop(self.remove_col, axis = 1)\n",
        "      return self.useful_data\n",
        "    except Exception as err:\n",
        "      raise Exception()\n",
        "  \n",
        "  def separate_label_feature(self, data, label_column_name):\n",
        "    try:\n",
        "      self.X = data.drop(labels=label_column_name,axis=1)\n",
        "      self.y = data[label_column_name]\n",
        "      return self.X, self.y\n",
        "    except Exception as e:\n",
        "      raise Exception()\n",
        "\n",
        "  def isnull_present(self, data):\n",
        "    self.null_present = False\n",
        "    self.cols_with_missing_values = []\n",
        "    self.cols = data.columns\n",
        "    try:\n",
        "      self.null_counts = data.isnull().sum()\n",
        "      for i in range(len(self.null_counts)):\n",
        "        if self.null_counts[i] > 0 :\n",
        "          self.null_present = True\n",
        "          self.cols_with_missing_values.append(self.cols[i])\n",
        "      return self.null_present, self.cols_with_missing_values\n",
        "    except Exception as err:\n",
        "      raise Exception()\n",
        "  \n",
        "  def impute_missing_values(self, data, cols_with_missing_values):\n",
        "    self.cols_with_missing_values = cols_with_missing_values\n",
        "    self.data = data\n",
        "    try:\n",
        "      self.Imputer = CategoricalImputer()\n",
        "      for col in self.cols_with_missing_values:\n",
        "        self.data[col] = self.Imputer.fit_transform(self.data[col])\n",
        "      return self.data\n",
        "    except Exception as err:\n",
        "      raise Exception()\n",
        "  \n",
        "  def scale_numerical_feature(self, data):\n",
        "    self.data = data\n",
        "    self.num_df = self.data[['months_as_customer', 'policy_deductable', 'umbrella_limit',\n",
        "                          'capital-gains', 'capital-loss', 'incident_hour_of_the_day',\n",
        "                          'number_of_vehicles_involved', 'bodily_injuries', 'witnesses', 'injury_claim', 'property_claim', 'vehicle_claim']]\n",
        "    try:\n",
        "      self.scaler = StandardScaler()\n",
        "      self.scaled_data = self.scaler.fit_transform(self.num_df)\n",
        "      self.scaled_num_df = pd.DataFrame(data=self.scaled_data, columns=self.num_df.columns,index=self.data.index)\n",
        "      self.data.drop(columns=self.scaled_num_df.columns, inplace=True)\n",
        "      self.data = pd.concat([self.scaled_num_df, self.data], axis=1)\n",
        "      return self.data\n",
        "    except Exception as err:\n",
        "      raise Exception()\n",
        "  def handle_imbalanced_dataset(self,x,y):\n",
        "    try:\n",
        "      self.os = SMOTETomek(0.75)\n",
        "      self.x_sampled,self.y_sampled = self.os.fit_sample(x,y)\n",
        "      return self.x_sampled,self.y_sampled\n",
        "    except Exception as err:\n",
        "      raise Exception()\n",
        "  def encode_catrgorical_coulmns(self, data):\n",
        "    self.data=data\n",
        "    try:\n",
        "      self.cat_df = self.data.select_dtypes(include=['object']).copy()\n",
        "      #print(self.cat_df.info())\n",
        "      self.cat_df['policy_csl'] = self.cat_df['policy_csl'].map({'100/300': 1, '250/500': 2.5, '500/1000': 5})\n",
        "      self.cat_df['insured_education_level'] = self.cat_df['insured_education_level'].map({'JD': 1, 'High School': 2, 'College': 3, 'Masters': 4, 'Associate': 5, 'MD': 6, 'PhD': 7}) # ordinalEncoding\n",
        "      self.cat_df['incident_severity'] = self.cat_df['incident_severity'].map({'Trivial Damage': 1, 'Minor Damage': 2, 'Major Damage': 3, 'Total Loss': 4})\n",
        "      self.cat_df['insured_sex'] = self.cat_df['insured_sex'].map({'FEMALE': 0, 'MALE': 1})\n",
        "      self.cat_df['property_damage'] = self.cat_df['property_damage'].map({'NO': 0, 'YES': 1})\n",
        "      self.cat_df['police_report_available'] = self.cat_df['police_report_available'].map({'NO': 0, 'YES': 1})\n",
        "      try:\n",
        "        self.cat_df['fraud_reported'] = self.cat_df['fraud_reported'].map({'N': 0, 'Y': 1}) # only during the training state.\n",
        "        self.cols_to_drop=['policy_csl', 'insured_education_level', 'incident_severity', 'insured_sex', 'property_damage', 'police_report_available', 'fraud_reported']\n",
        "      except Exception as err:\n",
        "        self.cols_to_drop=['policy_csl', 'insured_education_level', 'incident_severity', 'insured_sex', 'property_damage', 'police_report_available', 'fraud_reported']\n",
        "      \n",
        "      for col in self.cat_df.drop(columns=self.cols_to_drop).columns:\n",
        "        self.cat_df = pd.get_dummies(self.cat_df, columns=[col], prefix=[col], drop_first=True) \n",
        "      self.data.drop(columns=self.data.select_dtypes(include=['object']).columns, inplace=True)\n",
        "      self.data= pd.concat([self.cat_df,self.data],axis=1)      \n",
        "      return self.data\n",
        "    except Exception as e:\n",
        "      raise Exception()      \n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRAU5MhU3WwJ"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.ensemble import BalancedBaggingClassifier \n",
        "from sklearn.metrics  import roc_auc_score,accuracy_score, confusion_matrix, classification_report"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqGgZm7g2NxC"
      },
      "source": [
        "class Model_Finder:\n",
        "  def __init__(self):\n",
        "    self.sv_classifier=SVC()\n",
        "    self.xgb = XGBClassifier(objective='binary:logistic',n_jobs=-1) \n",
        "    self.RF = RandomForestClassifier()\n",
        "  def get_best_params_for_xgboost(self, train_x, train_y):\n",
        "    try:\n",
        "      self.param_grid_xgboost = { \"n_estimators\": [100, 130], \"criterion\": ['gini', 'entropy'], \"max_depth\": range(8, 10, 1) }\n",
        "      self.grid= GridSearchCV(XGBClassifier(objective='binary:logistic'),self.param_grid_xgboost, verbose=3,cv=10)\n",
        "      self.grid.fit(train_x, train_y)\n",
        "      self.criterion = self.grid.best_params_['criterion']\n",
        "      self.max_depth = self.grid.best_params_['max_depth']\n",
        "      self.n_estimators = self.grid.best_params_['n_estimators']\n",
        "      self.xgb = XGBClassifier(criterion=self.criterion, max_depth=self.max_depth,n_estimators= self.n_estimators, n_jobs=-1 )\n",
        "      self.xgb.fit(train_x, train_y)\n",
        "      return self.xgb\n",
        "    except Exception as e:\n",
        "      raise Exception()\n",
        "  \n",
        "  def get_best_params_for_svm(self, train_x, train_y):\n",
        "    try:\n",
        "      self.param_grid = { \"kernel\": ['rbf', 'sigmoid'], \"C\": [0.1, 0.5, 1.0], \"random_state\": [0, 100, 200, 300] }\n",
        "      self.grid = GridSearchCV(estimator=self.sv_classifier, param_grid=self.param_grid, cv=10,  verbose=3)  \n",
        "      self.grid.fit(train_x, train_y)\n",
        "      self.kernel = self.grid.best_params_['kernel']\n",
        "      self.C = self.grid.best_params_['C']\n",
        "      self.random_state = self.grid.best_params_['random_state']\n",
        "      self.sv_classifier = SVC(kernel=self.kernel, C=self.C, random_state=self.random_state)\n",
        "      self.sv_classifier.fit(train_x, train_y)\n",
        "      return self.sv_classifier\n",
        "    except Exception as e:\n",
        "      raise Exception()\n",
        "\n",
        "  def get_best_params_for_RF(self, train_x, train_y):\n",
        "    try:\n",
        "      Number of trees in random forest\n",
        "      n_estimators = [int(x) for x in np.linspace(start = 100, stop = 700, num = 5)]\n",
        "      # Number of features to consider at every split\n",
        "      max_features = ['auto', 'sqrt']\n",
        "      # Maximum number of levels in tree\n",
        "      max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
        "      # max_depth.append(None)\n",
        "      # Minimum number of samples required to split a node\n",
        "      min_samples_split = [2, 5, 10, 15, 100]\n",
        "      # Minimum number of samples required at each leaf node\n",
        "      min_samples_leaf = [1, 2, 5, 10]\n",
        "      self.random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf}\n",
        "      self.grid = GridSearchCV(estimator=self.RF, param_grid=self.random_grid, cv = 5,  verbose=3)  \n",
        "      self.grid.fit(train_x, train_y)\n",
        "      self.n_estimators = self.grid.best_params_[\"n_estimators\"]\n",
        "      self.max_features = self.grid.best_params_[\"max_features\"]\n",
        "      self.max_depth = self.grid.best_params_[\"max_depth\"]\n",
        "      self.min_samples_split = self.grid.best_params_[\"min_samples_split\"]\n",
        "      self.min_samples_leaf = self.grid.best_params_[\"min_samples_leaf\"]\n",
        "      self.rf_classifier = RandomForestClassifier(n_estimators = self.n_estimators,max_features= self.max_features, max_depth = self.max_depth,\n",
        "                                                  min_samples_split = self.min_samples_split, min_samples_leaf = self.min_samples_leaf)\n",
        "      self.rf_classifier.fit(train_x, train_y)\n",
        "      return self.rf_classifier\n",
        "    except Exception as e:\n",
        "      raise Exception\n",
        "\n",
        "  def get_best_imbalance_model(self, train_x, test_x, train_y, test_y):\n",
        "     self.rf_balanced_classifier = BalancedBaggingClassifier(base_estimator = self.RF,\n",
        "                                 sampling_strategy = 'auto',\n",
        "                                 replacement = False,\n",
        "                                 random_state = 0)\n",
        "     self.rf_balanced_classifier.fit(train_x, train_y)\n",
        "     self.prediction_RF_bal = self.rf_balanced_classifier.predict(test_x)\n",
        "     if len(test_y.unique()) == 1: \n",
        "       self.rf_balance_score = accuracy_score(test_y,self.prediction_RF_bal)\n",
        "     else:\n",
        "       self.rf_balance_score = roc_auc_score(test_y, self.prediction_RF_bal)\n",
        "     return 'BalancedClassifier', self.rf_balanced_classifier, self.rf_balance_score\n",
        "\n",
        "  def get_best_model(self, train_x, test_x, train_y, test_y):\n",
        "    try:\n",
        "      self.xgboost= self.get_best_params_for_xgboost(train_x,train_y)\n",
        "      self.prediction_xgboost = self.xgboost.predict(test_x)\n",
        "      if len(test_y.unique()) == 1: #if there is only one label in y, then roc_auc_score returns error. We will use accuracy in that case\n",
        "          self.xgboost_score = accuracy_score(test_y, self.prediction_xgboost)\n",
        "      else:\n",
        "          self.xgboost_score = roc_auc_score(test_y, self.prediction_xgboost) # AUC for XGBoost \n",
        "      \n",
        "      self.svm=self.get_best_params_for_svm(train_x,train_y)\n",
        "      self.prediction_svm=self.svm.predict(test_x) # prediction using the SVM Algorithm\n",
        "\n",
        "      if len(test_y.unique()) == 1: #if there is only one label in y, then roc_auc_score returns error. We will use accuracy in that case\n",
        "          self.svm_score = accuracy_score(test_y,self.prediction_svm)\n",
        "      else:\n",
        "          self.svm_score = roc_auc_score(test_y, self.prediction_svm) # AUC for Random Forest\n",
        "      \n",
        "      self.RF = self.get_best_params_for_RF(train_x,train_y)\n",
        "      self.prediction_RF=self.svm.predict(test_x)\n",
        "\n",
        "      if len(test_y.unique()) == 1: \n",
        "          self.rf_score = accuracy_score(test_y,self.prediction_RF)\n",
        "      else:\n",
        "          self.rf_score = roc_auc_score(test_y, self.prediction_RF) # AUC for Random Forest\n",
        "\n",
        "      print(\"Prediction completed\")\n",
        "\n",
        "      lst = [self.svm_score, self.xgboost_score, self.rf_score] \n",
        "      good_score = max(lst)\n",
        "      #comparing the two models\n",
        "      if self.svm_score == good_score:\n",
        "          return 'SVM', self.sv_classifier, self.svm_score\n",
        "      elif self.xgboost_score == good_score:\n",
        "          return 'XGBoost', self.xgboost, self.xgboost_score\n",
        "      else:\n",
        "        return 'Random Forest', self.RF, self.rf_score\n",
        "    except Exception as e:\n",
        "      raise Exception()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciyZUvQJ67o6"
      },
      "source": [
        "import pickle\n",
        "from collections import Counter"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4VB6uUdwykI"
      },
      "source": [
        "class TrainModel:\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "  def trainingModel(self):\n",
        "    try:\n",
        "      preprocessor = Preprocessor(self.data)\n",
        "      data = preprocessor.remove_columns(self.data, columns = ['policy_number','_c39','policy_bind_date','policy_state','insured_zip','incident_location','incident_date','incident_state','incident_city','insured_hobbies','auto_make','auto_model','auto_year','age','total_claim_amount'])\n",
        "      data.replace(\"?\", np.nan, inplace = True)\n",
        "      is_null_present, cols_with_missing_values = preprocessor.isnull_present(data)\n",
        "      if (is_null_present):\n",
        "        data = preprocessor.impute_missing_values(data, cols_with_missing_values)\n",
        "      data = preprocessor.encode_catrgorical_coulmns(data)\n",
        "      X,Y = preprocessor.separate_label_feature(data,label_column_name='fraud_reported') \n",
        "      self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, Y, test_size = 0.25 , random_state = 0)\n",
        "      # print(self.X_train.columns, self.X_test.columns)\n",
        "      self.X_train = preprocessor.scale_numerical_feature(self.X_train)\n",
        "      self.X_test  = preprocessor.scale_numerical_feature(self.X_test)    \n",
        "      print(\"The number of classes before fit {}\".format(Counter(self.y_train)))\n",
        "      model_finder = Model_Finder() \n",
        "      balanced_name, balanced_model, balanced_score = model_finder.get_best_imbalance_model(self.X_train, self.X_test, self.y_train, self.y_test)\n",
        "      self.X_train, self.y_train = preprocessor.handle_imbalanced_dataset(self.X_train, self.y_train)\n",
        "      print(\"The number of classes after fit {}\".format(Counter(self.y_train)))          \n",
        "      best_model_name, best_model, best_score= model_finder.get_best_model(self.X_train, self.X_test.values, self.y_train, self.y_test) \n",
        "      print(\"Prediction completed\")\n",
        "      if best_score < balanced_score:\n",
        "        best_model_name = balanced_name\n",
        "        best_model = balanced_model\n",
        "        best_score = balanced_score\n",
        "      print(\"best model is {} \".format(best_model_name))\n",
        "      filename = '/content/drive/MyDrive/Insurance Fraud Detection/best_model_{}.sav'.format(best_model_name)\n",
        "      pickle.dump(best_model, open(filename, 'wb'))    \n",
        "      loaded_model = pickle.load(open(filename, 'rb'))\n",
        "      y_predict = loaded_model.predict(self.X_test.values)\n",
        "      print(confusion_matrix(self.y_test,y_predict))\n",
        "      print(accuracy_score(self.y_test,y_predict))\n",
        "      print(classification_report(self.y_test,y_predict))\n",
        "    except Exception as e:\n",
        "      raise Exception()\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JFT8HpKwysX"
      },
      "source": [
        "try:\n",
        "  data = pd.read_csv(\"/content/drive/MyDrive/Insurance Fraud Detection/insurance_claims.csv\")\n",
        "  trainModelObj = TrainModel(data)\n",
        "  trainModelObj.trainingModel()\n",
        "except Exception as e:\n",
        "  raise Exception()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}